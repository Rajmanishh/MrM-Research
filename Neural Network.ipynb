{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_train_images():\n",
    "    with open(r\"D:\\OneDrive\\Desktop\\College\\MrM Research\\Coding\\archive\\train-images-idx3-ubyte\", 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    return data.reshape(-1, 28, 28)\n",
    "\n",
    "def load_train_labels():\n",
    "    with open(r\"D:\\OneDrive\\Desktop\\College\\MrM Research\\Coding\\archive\\train-labels-idx1-ubyte\", 'rb') as f:\n",
    "        return np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "def load_test_images():\n",
    "    with open(r\"D:\\OneDrive\\Desktop\\College\\MrM Research\\Coding\\archive\\t10k-images-idx3-ubyte\", 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    return data.reshape(-1, 28, 28)\n",
    "\n",
    "def load_test_labels():\n",
    "    with open(r\"D:\\OneDrive\\Desktop\\College\\MrM Research\\Coding\\archive\\t10k-labels-idx1-ubyte\", 'rb') as f:\n",
    "        return np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "X_train = load_train_images()\n",
    "y_train = load_train_labels()\n",
    "\n",
    "X_test = load_test_images()\n",
    "y_test = load_test_labels()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(class_names[y_train[i]])\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "X = X_train.reshape(-1, 28*28)\n",
    "X_test_flat = X_test.reshape(-1, 28*28)\n",
    "\n",
    "\n",
    "def he_init(fan_in, fan_out):\n",
    "    return np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)\n",
    "# hyperparameters\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "dropout_rate = 0.2\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "l2_lambda= 1e-4\n",
    "# He initialization\n",
    "W1 = he_init(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "W2 = he_init(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, relu(x))\n",
    "plt.title(\"ReLU Activation\")\n",
    "plt.show()\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "def cross_entropy(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(np.log(y_pred[np.arange(m), y_true] + 1e-8)) / m\n",
    "\n",
    "#forward\n",
    "def forward_prop(X, params):\n",
    "       # 20% neurons dropped\n",
    "\n",
    "    W1, b1 = params[\"W1\"], params[\"b1\"]\n",
    "    W2, b2 = params[\"W2\"], params[\"b2\"]\n",
    "\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = relu(Z1)\n",
    "    dropout_mask = (np.random.rand(*A1.shape) > dropout_rate)\n",
    "    A1 = (A1 * dropout_mask) / (1 - dropout_rate)\n",
    "\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    cache = {\n",
    "    \"X\": X,\n",
    "    \"Z1\": Z1,\n",
    "    \"A1\": A1,\n",
    "    \"Z2\": Z2,\n",
    "    \"A2\": A2,\n",
    "    \"dropout_mask\": dropout_mask\n",
    "    }\n",
    "\n",
    "\n",
    "    return A2, cache\n",
    "\n",
    "\n",
    "def backward_prop(y, params, cache):\n",
    "    W2 = params[\"W2\"]\n",
    "    X = cache[\"X\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "\n",
    "    m = y.shape[0]\n",
    "    # Output layer gradient\n",
    "    dZ2 = A2.copy()\n",
    "    dZ2[np.arange(m), y] -= 1\n",
    "    dZ2 /= m\n",
    "    dW2 = A1.T @ dZ2 + l2_lambda * W2\n",
    "    dW1 = X.T @ dZ1 + l2_lambda * params[\"W1\"]\n",
    "\n",
    "\n",
    "    # Hidden layer gradient\n",
    "    dA1 = dZ2 @ W2.T\n",
    "    dA1 = dA1 * cache[\"dropout_mask\"] / (1 - dropout_rate)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "\n",
    "    dW1 = X.T @ dZ1 + l2_lambda * params[\"W1\"]\n",
    "    db1 = np.sum(dZ1, axis=0)\n",
    "    db2 = np.sum(dZ2, axis=0)\n",
    "\n",
    "\n",
    "    grads = {\n",
    "        \"W1\": dW1, \"b1\": db1,\n",
    "        \"W2\": dW2, \"b2\": db2\n",
    "    }\n",
    "\n",
    "    return grads\n",
    "\n",
    "#optimiser\n",
    "def init_adam(params):\n",
    "    adam = {}\n",
    "    for key in params:\n",
    "        adam[\"m_\" + key] = np.zeros_like(params[key])\n",
    "        adam[\"v_\" + key] = np.zeros_like(params[key])\n",
    "    adam[\"t\"] = 0\n",
    "    return adam\n",
    "def adam_update(params, grads, adam,\n",
    "                lr=0.001,\n",
    "                beta1=0.9,\n",
    "                beta2=0.999,\n",
    "                eps=1e-8):\n",
    "    \n",
    "    adam[\"t\"] += 1\n",
    "    t = adam[\"t\"]\n",
    "    \n",
    "    for key in params:\n",
    "        # Update biased first moment\n",
    "        adam[\"m_\" + key] = beta1 * adam[\"m_\" + key] + (1 - beta1) * grads[key]\n",
    "        \n",
    "        # Update biased second moment\n",
    "        adam[\"v_\" + key] = beta2 * adam[\"v_\" + key] + (1 - beta2) * (grads[key] ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = adam[\"m_\" + key] / (1 - beta1 ** t)\n",
    "        v_hat = adam[\"v_\" + key] / (1 - beta2 ** t)\n",
    "        \n",
    "        # Parameter update\n",
    "        params[key] -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2\n",
    "}\n",
    "\n",
    "\n",
    "adam = init_adam(params)\n",
    "\n",
    "#mini batch\n",
    "y = y_train\n",
    "\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Shuffle data\n",
    "    perm = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[perm]\n",
    "    y_shuffled = y[perm]\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "\n",
    "        X_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        A2, cache = forward_prop(X_batch, params)\n",
    "\n",
    "        # Loss\n",
    "        # Loss\n",
    "        data_loss = cross_entropy(y_batch, A2)\n",
    "        epoch_loss += data_loss\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        grads = backward_prop(y_batch, params, cache)\n",
    "\n",
    "        # Adam update\n",
    "        adam_update(params, grads, adam, lr=learning_rate)\n",
    "\n",
    "    epoch_loss /= max(1, num_samples // batch_size)\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        l2_loss = 0.5 * l2_lambda * (\n",
    "    np.sum(params[\"W1\"] ** 2) + np.sum(params[\"W2\"] ** 2)\n",
    ")\n",
    "        loss = cross_entropy(y_batch, A2)+l2_loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        grads = backward_prop(y_batch, params, cache)\n",
    "\n",
    "        # Adam update\n",
    "        adam_update(params, grads, adam, lr=learning_rate)\n",
    "\n",
    "    epoch_loss /= max(1, num_samples // batch_size)\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "l2_loss = 0.5 * l2_lambda * (\n",
    "    np.sum(params[\"W1\"] ** 2) + np.sum(params[\"W2\"] ** 2)\n",
    ")\n",
    "\n",
    "epoch_loss += l2_loss\n",
    "epoch_loss /= max(1, num_samples // batch_size)\n",
    "loss_history.append(epoch_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a1342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
