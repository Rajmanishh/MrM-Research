{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. Helper Functions (Splitting & Scaling)\n",
    "# ==========================================\n",
    "\n",
    "def train_test_split_manual(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Splits data into training and testing sets.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_samples = int(len(X) * test_size)\n",
    "    \n",
    "    test_idx = indices[:test_samples]\n",
    "    train_idx = indices[test_samples:]\n",
    "    \n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "def standardize_manual(X_train, X_test):\n",
    "    \"\"\"Standardizes features by removing the mean and scaling to unit variance.\"\"\"\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    std = np.where(std == 0, 1, std)\n",
    "    \n",
    "    X_train_scaled = (X_train - mean) / std\n",
    "    X_test_scaled = (X_test - mean) / std\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def f1_score_manual(y_true, y_pred):\n",
    "    \"\"\"Calculates F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def confusion_matrix_manual(y_true, y_pred):\n",
    "    \"\"\"Returns 2x2 confusion matrix [[TN, FP], [FN, TP]]\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. PCA Implementation from Scratch (updated)\n",
    "# ==========================================\n",
    "\n",
    "class PCA_Scratch:\n",
    "    \"\"\"\n",
    "    PCA implementation that:\n",
    "      - accepts variance_threshold (float in (0,1])\n",
    "      - sets: components (shape: n_components x n_features), mean, n_components\n",
    "      - also sets sklearn-like: explained_variance_ and explained_variance_ratio_\n",
    "    \"\"\"\n",
    "    def __init__(self, variance_threshold=0.95):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        # original names you used\n",
    "        self.components = None   # shape: (n_components, n_features)\n",
    "        self.mean = None\n",
    "        self.n_components = None\n",
    "        # sklearn-like attributes (added)\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2D array\")\n",
    "        # 1. Mean centering\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # 2. Covariance matrix\n",
    "        cov = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        # 3. Eigen decomposition (use eigh for symmetric matrix)\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov)  # ascending order\n",
    "\n",
    "        # 4. Sort eigenvalues & eigenvectors descending\n",
    "        idxs = np.argsort(eigvals)[::-1]\n",
    "        eigvals = eigvals[idxs]\n",
    "        eigvecs = eigvecs[:, idxs]   # columns are eigenvectors\n",
    "\n",
    "        # 5. Explained variance ratio (all components)\n",
    "        total_variance = np.sum(eigvals)\n",
    "        if total_variance <= 0:\n",
    "            explained_variance_ratio_all = np.zeros_like(eigvals)\n",
    "        else:\n",
    "            explained_variance_ratio_all = eigvals / total_variance\n",
    "\n",
    "        # 6. Determine number of components to keep by variance_threshold\n",
    "        vt = 1.0 if self.variance_threshold is None else float(self.variance_threshold)\n",
    "        if not (0 < vt <= 1):\n",
    "            raise ValueError(\"variance_threshold must be in (0, 1]\")\n",
    "\n",
    "        cumulative = np.cumsum(explained_variance_ratio_all)\n",
    "        k = int(np.searchsorted(cumulative, vt, side='left') + 1)\n",
    "        k = min(k, eigvals.shape[0])\n",
    "        self.n_components = k\n",
    "\n",
    "        # 7. Store components as (k, n_features) to match your transform use\n",
    "        self.components = eigvecs[:, :k].T.copy()  # rows = components\n",
    "\n",
    "        # 8. Store explained variance info (kept full vectors for scree flexibility)\n",
    "        self.explained_variance_ = eigvals.copy()\n",
    "        self.explained_variance_ratio_ = explained_variance_ratio_all.copy()\n",
    "\n",
    "        print(f\"PCA: Selected {self.n_components} components explaining {cumulative[self.n_components-1]:.2%} variance.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.mean is None or self.components is None:\n",
    "            raise ValueError(\"PCA_Scratch not fitted. Call fit(X) first.\")\n",
    "        X_centered = np.asarray(X, dtype=float) - self.mean\n",
    "        # components.T shape = (n_features, k) -> dot gives (n_samples, k)\n",
    "        return np.dot(X_centered, self.components.T)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_pca):\n",
    "        if self.components is None or self.mean is None:\n",
    "            raise ValueError(\"PCA_Scratch not fitted. Call fit(X) first.\")\n",
    "        return np.dot(X_pca, self.components.T) + self.mean\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. SVM Implementation from Scratch\n",
    "# ==========================================\n",
    "\n",
    "class SVM_Scratch:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Convert labels to {-1, 1}\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient Descent (stochastic-like loop)\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Hinge loss condition: y_i * (w.x_i - b) >= 1\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                \n",
    "                if condition:\n",
    "                    # Gradient if correctly classified (only regularization)\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    # Gradient if misclassified (regularization + loss)\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        # Convert sign back to {0, 1}\n",
    "        return np.where(np.sign(approx) == -1, 0, 1)\n",
    "    \n",
    "\n",
    "# ==========================================\n",
    "# 4. Logistic Regression from Scratch\n",
    "# ==========================================\n",
    "\n",
    "class LogisticRegression_Scratch:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Gradient calculation\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return (y_predicted > 0.5).astype(int)\n",
    "    \n",
    "\n",
    "# ==========================================\n",
    "# 5. Visualization Functions (robust)\n",
    "# ==========================================\n",
    "\n",
    "def plot_scree(pca):\n",
    "    \"\"\"Scree plot that uses explained_variance_ratio_ if present.\"\"\"\n",
    "    if hasattr(pca, \"explained_variance_ratio_\") and pca.explained_variance_ratio_ is not None:\n",
    "        ratios = np.asarray(pca.explained_variance_ratio_)\n",
    "    elif hasattr(pca, \"explained_variance_\") and pca.explained_variance_ is not None:\n",
    "        ev = np.asarray(pca.explained_variance_)\n",
    "        total = np.sum(ev)\n",
    "        ratios = ev / total if total > 0 else np.zeros_like(ev)\n",
    "    else:\n",
    "        raise AttributeError(\"PCA object has no explained_variance_ratio_ or explained_variance_\")\n",
    "\n",
    "    comps = np.arange(1, len(ratios) + 1)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(comps, ratios, marker='o', linestyle='--')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.xticks(comps)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pca_scatter(X_pca, y, title='PCA Scatter Plot'):\n",
    "    X_pca = np.asarray(X_pca)\n",
    "    if X_pca.ndim != 2:\n",
    "        raise ValueError(\"X_pca must be 2D\")\n",
    "    # If only 1 component available, plot 1D jittered scatter\n",
    "    if X_pca.shape[1] == 1:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        jitter = (np.random.rand(X_pca.shape[0]) - 0.5) * 0.01\n",
    "        plt.scatter(X_pca[:, 0], jitter, c=y, cmap='viridis', edgecolor='k', alpha=0.7)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.yticks([])\n",
    "        plt.title(title + \" (1D)\")\n",
    "        plt.colorbar(label='Target Class')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    # If >=2 components\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(label='Target Class')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(x=j, y=i, s=str(int(cm[i, j])), va='center', ha='center', size='xx-large')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6. Main Execution\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Data Load & Preprocess ---\n",
    "    try:\n",
    "        df = pd.read_csv(r\"D:\\OneDrive\\Desktop\\College\\MrM Research\\Coding\\Data\\titanic.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'titanic.csv' not found.\")\n",
    "        exit()\n",
    "\n",
    "    # Basic imputation & encoding (your original steps)\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    # drop columns not used\n",
    "    to_drop = ['Cabin', 'PassengerId', 'Name', 'Ticket']\n",
    "    df = df.drop([c for c in to_drop if c in df.columns], axis=1, errors='ignore')\n",
    "\n",
    "    if 'Sex' in df.columns:\n",
    "        df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    if 'Embarked' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "    # if Survived not present, attempt to find alternate label column\n",
    "    if 'Survived' not in df.columns:\n",
    "        raise RuntimeError(\"Dataset must contain 'Survived' column\")\n",
    "\n",
    "    X = df.drop('Survived', axis=1).values\n",
    "    y = df['Survived'].values.astype(int)\n",
    "\n",
    "    # Split & Scale\n",
    "    X_train, X_test, y_train, y_test = train_test_split_manual(X, y)\n",
    "    X_train_scaled, X_test_scaled = standardize_manual(X_train, X_test)\n",
    "\n",
    "    # --- PCA Phase ---\n",
    "    print(\"\\n--- 1. PCA Analysis ---\")\n",
    "    pca = PCA_Scratch(variance_threshold=0.95)  # keep enough components to explain 95% variance\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    # Visualizations\n",
    "    plot_scree(pca)\n",
    "\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    plot_pca_scatter(X_train_pca, y_train, title=\"PCA Scatter (Training Data)\")\n",
    "\n",
    "    # --- Model Training ---\n",
    "    print(\"\\n--- 2. Model Training (on PCA features) ---\")\n",
    "\n",
    "    # SVM\n",
    "    print(\"Training SVM...\")\n",
    "    svm = SVM_Scratch(learning_rate=0.001, lambda_param=0.01, n_iters=2000)\n",
    "    svm.fit(X_train_pca, y_train)\n",
    "    svm_pred = svm.predict(X_test_pca)\n",
    "\n",
    "    # Logistic Regression\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    lr = LogisticRegression_Scratch(learning_rate=0.05, n_iters=2000)\n",
    "    lr.fit(X_train_pca, y_train)\n",
    "    lr_pred = lr.predict(X_test_pca)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"\\n--- 3. Evaluation Results ---\")\n",
    "\n",
    "    # SVM Metrics\n",
    "    svm_acc = accuracy(y_test, svm_pred)\n",
    "    svm_f1 = f1_score_manual(y_test, svm_pred)\n",
    "    svm_cm = confusion_matrix_manual(y_test, svm_pred)\n",
    "\n",
    "    # LR Metrics\n",
    "    lr_acc = accuracy(y_test, lr_pred)\n",
    "    lr_f1 = f1_score_manual(y_test, lr_pred)\n",
    "    lr_cm = confusion_matrix_manual(y_test, lr_pred)\n",
    "\n",
    "    print(f\"{'Metric':<20} {'SVM':<15} {'Logistic Regression':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Accuracy':<20} {svm_acc:<15.4f} {lr_acc:<15.4f}\")\n",
    "    print(f\"{'F1 Score':<20} {svm_f1:<15.4f} {lr_f1:<15.4f}\")\n",
    "\n",
    "    # Plot Confusion Matrices\n",
    "    print(\"\\nDisplaying Confusion Matrices...\")\n",
    "    plot_confusion_matrix(svm_cm, title=\"SVM Confusion Matrix\")\n",
    "    plot_confusion_matrix(lr_cm, title=\"Logistic Regression Confusion Matrix\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
